{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Code</b>\n",
    "\n",
    "The code is all in agent.py and consists of modifications in the update function, as well as three new functions: get_actions, get_max and init_qt.\n",
    "\n",
    "<b>Implement a Basic Driving Agent</b>\n",
    "\n",
    "<b>QUESTION:</b> Observe what you see with the agent's behavior as it takes random actions. Does the smartcab eventually make it to the destination? Are there any other interesting observations to note?\n",
    "\n",
    "When taking random actions, the agent goes all over the place and also sometimes racks up significant negative penalties. It rarely makes it to the destination. Out of 10 runs, it only once made it to the destination. Another interesting observation is that the performance of the smartcab does not improve over time.\n",
    "\n",
    "<b>Inform the Driving Agent</b>\n",
    "\n",
    "<b>QUESTION:</b> What states have you identified that are appropriate for modeling the smartcab and environment? Why do you believe each of these states to be appropriate for this problem?\n",
    "\n",
    "The elements of the state identified include the waypoint / GPS direction, the traffic light, and the directions / existence of traffic coming from each of the three directions (oncoming, left, right). Those are five elements total. These elements are appropriate because they are all relevant to rewards at the next state -- either through going in the right direction (waypoint), or obeying traffic laws (lights and traffic).\n",
    "\n",
    "<b>OPTIONAL:</b> How many states in total exist for the smartcab in this environment? Does this number seem reasonable given that the goal of Q-Learning is to learn and make informed decisions about each state? Why or why not?\n",
    "\n",
    "The total number of states is 384 -- 3 (waypoints) x 2 (lights) x (4 (traffic states) x 3 (traffic directions)). This, in turn, is multiplied by four, which is the number of actions possible at each state, meaning that there are 1,537 Q-values to learn. This is quite a large number of states and we'll have to do a large number of iterations to get meaningful values for all of these.\n",
    "\n",
    "<b>Implement a Q-Learning Driving Agent</b>\n",
    "\n",
    "<b>QUESTION:</b> What changes do you notice in the agent's behavior when compared to the basic driving agent when random actions were always taken? Why is this behavior occurring?\n",
    "\n",
    "The agent tends to follow traffic laws and tends to follow the waypoint direction as long as the path is clear. The reason is that it is naively tring to maximize the next step's reward.\n",
    "\n",
    "<b>Improve the Q-Learning Driving Agent</b>\n",
    "\n",
    "<b>QUESTION:</b> Report the different values for the parameters tuned in your basic implementation of Q-Learning. For which set of parameters does the agent perform best? How well does the final driving agent perform?\n",
    "\n",
    "I have set the number of trials to 1000. When alpha = .1; gamma = 0.8; epsilon = .2, the last 10 trials always succeed. When alpha = .5, however, the last 10 trails all fail. When learning too fast, we lose too much of our previous information and don't succeed in coming up with a good policy. \n",
    "\n",
    "<b>QUESTION:</b> Does your agent get close to finding an optimal policy, i.e. reach the destination in the minimum possible time, and not incur any penalties? How would you describe an optimal policy for this problem?\n",
    "\n",
    "With the right parameters, it does appear that we come up with a good policy. Most generally, the best policy first obeys the traffic rules (avoids any penalties) and then takes direction from the waypoint (minimizing time).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
